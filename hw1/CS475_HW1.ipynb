{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS475 Natural Language Model - Programming Assignment 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission Guide"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This homework is built on top of Micrograd by Andrej Karpathy. He has an excellent [tutorial](https://youtu.be/VMj-3S1tku0?si=EpsUsPGzZs5mIBMi) going over the codebase and building it from scratch. You should have no difficulty solving this homework if you understand that tutorial.\n",
    "\n",
    "**How to submit**\n",
    "* Fill out <mark>TODO</mark> blocks, **DO NOT** modify other parts of the skeleton code.\n",
    "* Submit one file: hw1_{student_ID}.ipynb to KLMS\n",
    "    e.g.hw1_20243150.ipynb\n",
    "* **Late submission policy**: After the submission deadline, you will\n",
    "immediately lose 20% of the score, another 20% after 24 hours later, and so on.\n",
    "Submission after 72 hours (3 days) will not be counted. However, you can use late days for this assignment, for which you have to send an email to inform the TAs. See course syllabus website Late Policy section.\n",
    "\n",
    "**Note**\n",
    "* Make a copy of this .ipynb file. Do not directly edit this file.\n",
    "* You are required to use numpy, do not use neither pytorch nor tensorflow.\n",
    "* Check whether your whole cells work well by restarting runtime code and running all before the submission.\n",
    "* TA will look into the implemented functions, their validity and give corresponding score to each <mark>TODO</mark> problem.\n",
    "* Ask questions through Slack so that you can share information with other students.\n",
    "* TA in charge: Yeahoon Kim (yeahoon.kim@kaist.ac.kr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In this programming assignment, you will**\n",
    "* Learn how the backporpagation works.\n",
    "* Learn how the pytorch module works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import cell\n",
    "import math\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "from graphviz import Digraph\n",
    "%matplotlib inline\n",
    "\n",
    "# set seed\n",
    "seed_value = 42\n",
    "random.seed(seed_value)\n",
    "np.random.seed(seed_value)\n",
    "torch.manual_seed(seed_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Derivation of a simple function with one input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. What is derivation?\n",
    "\n",
    "Derivation is a mathematical process used to find the rate of change of a function with respect to its input variables. It is an essential tool in calculus and is widely used in various fields of science and engineering.\n",
    "\n",
    "In programming, we can approximate the derivative of a function by using numerical methods. One common method is called the **\"finite difference approximation.\"**\n",
    "\n",
    "The finite difference approximation calculates the derivative by estimating the slope of the function at a given point. It does this by taking the difference between the function values at two nearby points and dividing it by the difference in their corresponding input values. This can be expressed as an equation as follows:\n",
    "\n",
    "$\\frac{f(x+h)-f(x)}{h}$\n",
    "\n",
    "You can check the detail in [here](https://en.wikipedia.org/wiki/Finite_difference)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Simple example of the derivation\n",
    "The following is the basic example of the derivation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "################## TODO: Implement the gradient function ##################\n",
    "# The gradient function should take a function f and a point x and return the gradient of f at x.\n",
    "# The gradient should be approximated using finite differences with a small step size h.\n",
    "# The function f is assumed to be scalar-valued.\n",
    "def gradient(f, x: float, h=1e-5) -> float:\n",
    "    \n",
    "    \n",
    "    return 0\n",
    "###########################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function f(x)\n",
    "def f(x: float) -> float:\n",
    "    return 3 * x**2 - 4 * x + 5\n",
    "\n",
    "# Find the gradient of f(x) at x = 2\n",
    "x = 2\n",
    "df = gradient(f, x)\n",
    "\n",
    "print(f\"Gradient of function f at the point {x}: {df:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = np.arange(-5, 5, 0.25)\n",
    "ys = f(xs)\n",
    "plt.plot(xs, ys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Implement backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Implement value object for the gradient flow\n",
    "The `Value` object is a class that represents a scalar value and its gradient. It is used in the context of gradient descent and backpropagation algorithms. The `Value` object has the following attributes and methods:\n",
    "\n",
    "**Attributes:**\n",
    "- `data`: The scalar value stored in the object.\n",
    "- `grad`: The gradient of the scalar value with respect to some variable.\n",
    "- `_backward`: A function that defines the backward pass of the backpropagation algorithm.\n",
    "- `_prev`: A set of `Value` objects that are the inputs to the operation that produced this object.\n",
    "- `_op`: A string that represents the operation that produced this object.\n",
    "\n",
    "**Methods:**\n",
    "- `__add__(self, other)`: Overloads the `+` operator to perform addition between two `Value` objects or a `Value` object and a scalar.\n",
    "- `__mul__(self, other)`: Overloads the `*` operator to perform multiplication between two `Value` objects or a `Value` object and a scalar.\n",
    "- `__pow__(self, other)`: Overloads the `**` operator to perform exponentiation between a `Value` object and an integer or float.\n",
    "- `relu(self)`: Applies the rectified linear unit (ReLU) activation function to the `Value` object.\n",
    "- `backward(self)`: Performs the backward pass of the backpropagation algorithm to compute the gradients of the `Value` object and its inputs.\n",
    "\n",
    "The `Value` object is used in the implementation of neural networks and other machine learning algorithms to compute gradients and update the model parameters during training. It allows for automatic differentiation and efficient computation of gradients using the chain rule."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<mark>TODO</mark> Implement the activation functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Value:\n",
    "    \"\"\" stores a single scalar value and its gradient \"\"\"\n",
    "\n",
    "    def __init__(self, data, _children=(), _op='', label=''):\n",
    "        self.data = data\n",
    "        self.grad = 0\n",
    "        # internal variables used for autograd graph construction\n",
    "        self._backward = lambda: None\n",
    "        self._prev = set(_children)\n",
    "        self._op = _op # the op that produced this node, for graphviz / debugging / etc\n",
    "        self.label = label\n",
    "\n",
    "    def __add__(self, other):\n",
    "        other = other if isinstance(other, Value) else Value(other)\n",
    "        out = Value(self.data + other.data, (self, other), '+')\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += out.grad\n",
    "            other.grad += out.grad\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "\n",
    "    def __mul__(self, other):\n",
    "        other = other if isinstance(other, Value) else Value(other)\n",
    "        out = Value(self.data * other.data, (self, other), '*')\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += other.data * out.grad\n",
    "            other.grad += self.data * out.grad\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "\n",
    "    def __pow__(self, other):\n",
    "        assert isinstance(other, (int, float)), \"only supporting int/float powers for now\"\n",
    "        out = Value(self.data**other, (self,), f'**{other}')\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += (other * self.data**(other-1)) * out.grad\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "\n",
    "    def log(self):\n",
    "        out = Value(math.log(self.data), (self,), 'log')\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += (1/self.data) * out.grad\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "    \n",
    "    def relu(self):\n",
    "        out = Value(0 if self.data < 0 else self.data, (self,), 'ReLU')\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += (out.data > 0) * out.grad\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "\n",
    "    ############# TODO: implement more activation functions ############\n",
    "    # You can implement more activations such as sigmoid, tanh, silu, gelu, elu etc.\n",
    "    # Make sure to implement their gradients as well.\n",
    "    # If there is an activation function you want to implement, implement it here.\n",
    "    \n",
    "    def sigmoid(self):\n",
    "        # You can refer to the following link to implement the sigmoid function\n",
    "        # https://en.wikipedia.org/wiki/Sigmoid_function\n",
    "        pass\n",
    "    \n",
    "    def tanh(self):\n",
    "        # You can refer to the following link to implement the tanh function\n",
    "        # https://en.wikipedia.org/wiki/Hyperbolic_function\n",
    "        pass\n",
    "    \n",
    "    def silu(self):\n",
    "        # You can refer to the following link to implement the SiLU function\n",
    "        # https://arxiv.org/abs/1702.03118\n",
    "        pass\n",
    "    \n",
    "    def gelu(self):\n",
    "        # You can refer to the following link to implement the GELU function\n",
    "        # https://arxiv.org/abs/1606.08415\n",
    "        pass\n",
    "    \n",
    "    def elu(self):\n",
    "        # You can refer to the following link to implement the ELU function\n",
    "        # https://arxiv.org/abs/1511.07289\n",
    "        pass\n",
    "    \n",
    "    ######################################################################\n",
    "    \n",
    "    def backward(self):\n",
    "\n",
    "        # topological order all of the children in the graph\n",
    "        topo = []\n",
    "        visited = set()\n",
    "        def build_topo(v):\n",
    "            if v not in visited:\n",
    "                visited.add(v)\n",
    "                for child in v._prev:\n",
    "                    build_topo(child)\n",
    "                topo.append(v)\n",
    "        build_topo(self)\n",
    "\n",
    "        # go one variable at a time and apply the chain rule to get its gradient\n",
    "        self.grad = 1\n",
    "        for v in reversed(topo):\n",
    "            v._backward()\n",
    "\n",
    "    def __neg__(self): # -self\n",
    "        return self * -1\n",
    "\n",
    "    def __radd__(self, other): # other + self\n",
    "        return self + other\n",
    "\n",
    "    def __sub__(self, other): # self - other\n",
    "        return self + (-other)\n",
    "\n",
    "    def __rsub__(self, other): # other - self\n",
    "        return other + (-self)\n",
    "\n",
    "    def __rmul__(self, other): # other * self\n",
    "        return self * other\n",
    "\n",
    "    def __truediv__(self, other): # self / other\n",
    "        return self * other**-1\n",
    "\n",
    "    def __rtruediv__(self, other): # other / self\n",
    "        return other * self**-1\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"Value(data={self.data}, grad={self.grad})\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sanity Check for the `sigmoid` and `tanh`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for checking the correctness of the implementation\n",
    "# You can also try the other activation functions you implemented; \n",
    "# note that sometimes framework's implementations differ slightly for numererical stability reasons. \n",
    "# You will be graded correct as long as your answer is reasonable.\n",
    "\n",
    "def test_sanity_check():\n",
    "    x = Value(-4.0)\n",
    "    z = 2 * x + 2 + x\n",
    "    q = z.tanh() + z * x\n",
    "    h = (z * z).sigmoid()\n",
    "    y = h + q + q * x\n",
    "    y.backward()\n",
    "    xmg, ymg = x, y\n",
    "\n",
    "    x = torch.Tensor([-4.0]).double()\n",
    "    x.requires_grad = True\n",
    "    z = 2 * x + 2 + x\n",
    "    q = F.tanh(z) + z * x\n",
    "    h = F.sigmoid(z * z)\n",
    "    y = h + q + q * x\n",
    "    y.backward()\n",
    "    xpt, ypt = x, y\n",
    "\n",
    "    # forward pass went well\n",
    "    assert ymg.data == ypt.data.item()\n",
    "    # backward pass went well\n",
    "    assert xmg.grad == xpt.grad.item()\n",
    "    \n",
    "test_sanity_check()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Implement Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's define a Multi-Layer Perceptron (MLP) using the previously defined `Value` object as the basic building block. The MLP will consist of multiple layers, where each layer performs a linear transformation followed by a non-linear activation function.\n",
    "\n",
    "### 3.1. Explanation of the MLP:\n",
    "- **Input Layer**: The input layer takes the raw input data and feeds it into the network.\n",
    "- **Hidden Layers**: Each hidden layer applies a linear transformation (i.e., a weighted sum of inputs) followed by a non-linear activation function (such as ReLU). These layers capture complex patterns in the data by progressively learning higher-level features.\n",
    "- **Output Layer**: The final layer outputs the predictions.\n",
    "\n",
    "In this setup, the `Value` objects will be used to perform all the necessary computations while tracking the gradients for backpropagation. This allows the MLP to learn by adjusting its weights based on the gradients computed during the backward pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Module:\n",
    "\n",
    "    def zero_grad(self):\n",
    "        for p in self.parameters():\n",
    "            p.grad = 0\n",
    "\n",
    "    def parameters(self):\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neuron(Module):\n",
    "\n",
    "    def __init__(self, feature_in, acti: str = 'relu'):\n",
    "        self.w = [Value(random.uniform(-1,1)) for _ in range(feature_in)]\n",
    "        self.b = Value(0)\n",
    "        self.acti = acti\n",
    "\n",
    "    def __call__(self, x):\n",
    "        act = sum((wi*xi for wi,xi in zip(self.w, x)), self.b)\n",
    "        if self.acti is None:\n",
    "            return act\n",
    "        elif hasattr(act, self.acti):\n",
    "            return getattr(act, self.acti)()\n",
    "        else:\n",
    "            print(\"Error: Activation function not defined\")\n",
    "            return None\n",
    "\n",
    "    def parameters(self):\n",
    "        return self.w + [self.b]\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"{self.acti} Neuron({len(self.w)})\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer(Module):\n",
    "\n",
    "    def __init__(self, nin, nout, acti, **kwargs):\n",
    "        self.neurons = [Neuron(nin, acti, **kwargs) for _ in range(nout)]\n",
    "\n",
    "    def __call__(self, x):\n",
    "        out = [n(x) for n in self.neurons]\n",
    "        return out[0] if len(out) == 1 else out\n",
    "\n",
    "    def parameters(self):\n",
    "        return [p for n in self.neurons for p in n.parameters()]\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"Layer of [{', '.join(str(n) for n in self.neurons)}]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(Module):\n",
    "\n",
    "    def __init__(self, nin, nouts, acti='relu'):\n",
    "        sz = [nin] + nouts\n",
    "        act = [acti] * (len(nouts)-1) + [None]\n",
    "        self.layers = [Layer(sz[i], sz[i + 1], acti=act[i]) for i in range(len(nouts))]\n",
    "\n",
    "    def __call__(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "    def parameters(self):\n",
    "        return [p for layer in self.layers for p in layer.parameters()]\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"MLP of [{', '.join(str(layer) for layer in self.layers)}]\"\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Neural Networks Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To ensure that the neural network is functioning correctly, we will evaluate its performance on a dummy dataset: approximating sin(x). \n",
    "\n",
    "Here we create sine function from -2 to 2 and then try to match the neural network's output with actual sine function's output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = np.linspace(-2 * np.pi, 2 * np.pi, 1000).reshape(-1, 1)\n",
    "y = np.sin(X)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Example usage\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"y_test shape:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Define model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<mark>TODO</mark> Implement the model using MLP. Make it reasonably large (but not too large) to deal with this particular dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "######## TODO: Define the model ########\n",
    "model = # write your model here\n",
    "\n",
    "########################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3. Define criterion\n",
    "For measuring the error between the predicted label and the true labels, you should define the loss function. There are several common loss functions you can use, depending on the type of problem you're solving:\n",
    "* **L1 Loss (Mean Absolute Error, MAE)**: This loss function calculates the absolute difference between the predicted and true values. It's commonly used in regression tasks where robustness to outliers is important.\n",
    "\n",
    "  $\\text{L1 Loss} = \\frac{1}{n} \\sum_{i=1}^{n} |\\text{prediction}_i - \\text{actual}_i|$\n",
    "\n",
    "You can see the details in [here](https://en.wikipedia.org/wiki/Mean_absolute_error)\n",
    "\n",
    "* **L2 Loss (Mean Squared Error, MSE)**: This loss function calculates the square of the difference between the predicted and true values. It's widely used in regression tasks because it penalizes larger errors more than smaller ones.\n",
    "\n",
    "  \n",
    "  $\\text{L2 Loss} = \\frac{1}{n} \\sum_{i=1}^{n} (\\text{prediction}_i - \\text{actual}_i)^2$\n",
    "  \n",
    "You can see the details in [here](https://en.wikipedia.org/wiki/Mean_squared_error)\n",
    "\n",
    "In addition, you can try various other loss functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3. Training Step\n",
    "<mark>TODO</mark> Implement the training step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_iterator(X, y, batch_size):\n",
    "  \"\"\"Yield batches of data.\"\"\"\n",
    "  for i in range(0, len(X), batch_size):\n",
    "      yield X[i:i + batch_size], y[i:i + batch_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######## TODO: ########\n",
    "gradient_rate = 1e-4  # change as necessary\n",
    "#######################\n",
    "\n",
    "num_epochs = 500 # change as necessary; but try to stay within 1000\n",
    "batch_size = 8 # experiment with this\n",
    "regul_coef = 1e-3 # change as necessary\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    for X_batch, y_batch in batch_iterator(X_train, y_train, batch_size):\n",
    "        y_pred = [model(data) for data in X_batch]\n",
    "        \n",
    "        ##### TODO: implement L1 and L2 loss. But while training, use only one of them and comment the other  #####\n",
    "        # implment L1 loss:\n",
    "\n",
    "        loss = None \n",
    "\n",
    "        # implement L2 loss:\n",
    "\n",
    "        loss = None \n",
    "\n",
    "\n",
    "        # TODO: Add L2 regularization to the loss with different regularization coefficients\n",
    "        # hint: think of model.parameters()\n",
    "        # note that in reality we do not regularize the bias terms, but for the sake of simplicity you can regularize them as well\n",
    "\n",
    "\n",
    "        ################################################\n",
    "        \n",
    "        model.zero_grad()\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        for p in model.parameters():\n",
    "            p.data -= gradient_rate * p.grad\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f'Epoch {epoch}, Loss: {loss.data}') # feel free to use other ways to track loss (eg different forms of averaging)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3. Evaluation\n",
    "Evaluate the model performance on holdout data (use L2 loss for all analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### TO-DO: Calculate the test loss #######"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting function. Visualize your results\n",
    "# this code assumes: print(X_test.shape, y_test.shape, y_pred.shape) --> (200, 1) (200, 1) (200,)\n",
    "\n",
    "sorted_indices = np.argsort(X_test[:, 0]) # sorting is neeed because we randomized the data before\n",
    "X_test_sorted = X_test[sorted_indices]\n",
    "y_test_sorted = y_test[sorted_indices]\n",
    "y_pred_sorted = np.array([y.data for y in y_pred])[sorted_indices]\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(X_test_sorted, y_test_sorted, label='True Values', color='blue')\n",
    "plt.plot(X_test_sorted, y_pred_sorted, label='Predicted Values', color='red', linestyle='--')\n",
    "plt.title('Sine Function Prediction')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('Y')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<mark>TODO</mark> Answer the following questions. Plot necessary graphs if needed.\n",
    "\n",
    "* Which loss function gives the better validation loss? Keep the number of epoch and learning rate same.\n",
    "* For the best loss function you find, which non-linearity performs the best?\n",
    "* Does your best working solution overfit? How to prevent overfitting? Does regularization above prevent overfitting?\n",
    "* How would you implement regularization without regularizing bias terms? Describe briefly in a paragraph. Be specific on what you will modify."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
